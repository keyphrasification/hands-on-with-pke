{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b764ffc9",
   "metadata": {},
   "source": [
    "# Hands-on session with pke - part 2\n",
    "\n",
    "This notebook provides a series of examples on how to parameterize the keyphrase extraction models in `pke`.\n",
    "More specifically, we will see how to customize the identification of keyphrase candidates and how to use different models implemented in `pke`.\n",
    "\n",
    "As a reminder, `pke` provides a standardized API for extracting keyphrases from a document by typing the following 5 lines:\n",
    "\n",
    "```python\n",
    "import pke\n",
    "\n",
    "extractor = pke.unsupervised.TfIdf()        # initialize a keyphrase extraction model, here TFxIDF\n",
    "extractor.load_document(input='text')       # load the content of the document (str or spacy Doc)\n",
    "extractor.candidate_selection()             # identify keyphrase candidates\n",
    "extractor.candidate_weighting()             # weight keyphrase candidates\n",
    "keyphrases = extractor.get_n_best(n=5)      # select the 5-best candidates as keyphrases\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d9d352",
   "metadata": {},
   "source": [
    "### Preamble - initializing a simple model and a sample document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "718afc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke\n",
    "\n",
    "# sample document (1895.abstr from the Inspec dataset)\n",
    "sample = \"\"\"An algorithm combining neural networks with fundamental parameters.\n",
    "An algorithm combining neural networks with the fundamental parameters equations (NNFP) is proposed for making\n",
    "corrections for non-linear matrix effects in x-ray fluorescence analysis. In the algorithm, neural networks were\n",
    "applied to relate the concentrations of components to both the measured intensities and the relative theoretical\n",
    "intensities calculated by the fundamental parameter equations. The NNFP algorithm is compared with the classical\n",
    "theoretical correction models, including the fundamental parameters approach, the Lachance-Traill model, a\n",
    "hyperbolic function model and the COLA algorithm. For an alloy system with 15 measured elements, in most cases,\n",
    "the prediction errors of the NNFP algorithm are lower than those of the fundamental parameters approach, the\n",
    "Lachance-Traill model, the hyperbolic function model and the COLA algorithm separately. If there are the serious\n",
    "matrix effects, such as matrix effects among Cr, Fe and Ni, the NNFP algorithm generally decreased predictive\n",
    "errors as compared with the classical models, except for the case of Cr by the fundamental parameters approach.\n",
    "The main reason why the NNFP algorithm has generally a better predictive ability than the classical theoretical\n",
    "correction models might be that neural networks can better calibrate the non-linear matrix effects in a complex\n",
    "multivariate system.\"\"\".replace(\"\\n\", \" \")\n",
    "\n",
    "# initialize a simple model that ranks candidates using their position\n",
    "extractor = pke.unsupervised.FirstPhrases()\n",
    "\n",
    "# load the document using the initialized model\n",
    "extractor.load_document(input=sample, language='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c9b2c",
   "metadata": {},
   "source": [
    "## Model parameterization - candidate selection\n",
    "\n",
    "Candidate selection is a crucial stage in keyphrase extraction as it determines the size of the search space (i.e. number of candidates to rank/weight) and the upper bound performance (i.e. maximum recall).\n",
    "Here, we will see how to configure the candidate selection method in `pke` to achieve the best compromise between search space and maximum performance.\n",
    "\n",
    "In order to compare candidate selection methods, we compute the maximum recall score against the gold-standard (human-assigned) keyphrases as\n",
    "\n",
    "$$max\\_recall = \\frac{| \\text{candidates} \\cap \\text{references}|}{|\\text{references}|}$$\n",
    "\n",
    "Candidate and reference keyphrases are stemmed (using `nltk`'s Porter stemmer) to reduce the number of mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb66d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold-standard keyphrases for the sample document (1895.abstr, keyphrases are in stemmed form)\n",
    "references = ['algorithm', 'neural network', 'fundament paramet', 'fundament paramet equat',\n",
    "              'nonlinear matrix effect', 'x-ray fluoresc analysi', 'intens', 'nnfp algorithm',\n",
    "              'theoret correct model', 'lachance-trail model', 'hyperbol function model',\n",
    "              'cola algorithm', 'alloy system', 'cr', 'fe', 'ni', 'complex multivari system']\n",
    "\n",
    "def max_recall(candidates, references):\n",
    "    return len(set(references) & set(candidates)) / len(set(references))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b3525",
   "metadata": {},
   "source": [
    "### Setting up a linguistic-based selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e00cf6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 keyphrase candidates were identified\n",
      "- Subsample of candidates: algorithm ; network ; paramet ; paramet equat ; nnfp\n",
      "- Maximum recall: 0.529\n",
      "- Missed reference keyphrases: {'fundament paramet equat', 'hyperbol function model', 'complex multivari system', 'theoret correct model', 'fundament paramet', 'neural network', 'lachance-trail model', 'nonlinear matrix effect'}\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"\"\"\n",
    "                NP:\n",
    "                    {<NOUN|PROPN>+}\n",
    "            \"\"\"\n",
    "\n",
    "extractor.grammar_selection(grammar=grammar)\n",
    "\n",
    "# let's see how many candidates are identified\n",
    "print(\"{} keyphrase candidates were identified\".format(len(extractor.candidates)))\n",
    "\n",
    "# print out a sample\n",
    "candidates = [*extractor.candidates]\n",
    "print(\"- Subsample of candidates:\", ' ; '.join(candidates[:5]))\n",
    "\n",
    "# compute the maximum recall\n",
    "print(\"- Maximum recall: {:.3f}\".format(max_recall(candidates, references)))\n",
    "\n",
    "# identify missed reference keyphrases\n",
    "missed = set(references) - set(candidates)\n",
    "print(\"- Missed reference keyphrases: {}\".format(missed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e242bc76",
   "metadata": {},
   "source": [
    "### <span style=\"background:lightpink\">Exercice ✍️</span>\n",
    "\n",
    "try modifying/adding PoS patterns of the grammar to increase the maximum recall, for example by allowing predicative adjectives (e.g. `<ADJ>+`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c502c893",
   "metadata": {},
   "source": [
    "### Setting up a n-gram-based selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74271ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173 keyphrase candidates were identified\n",
      "- Subsample of candidates: algorithm ; algorithm combin ; algorithm combin neural ; combin ; combin neural\n",
      "- Maximum recall: 0.941\n",
      "- Missed reference keyphrases: {'nonlinear matrix effect'}\n"
     ]
    }
   ],
   "source": [
    "# here we use a simple n-gram selection for candidates\n",
    "extractor.ngram_selection(n=3)\n",
    "\n",
    "# filter out spurious candidates \n",
    "for i, candidate in enumerate(list(extractor.candidates.keys())):\n",
    "    \n",
    "    # get the candidate words \n",
    "    words = [w.lower() for w in extractor.candidates[candidate].surface_forms[0]]\n",
    "    \n",
    "    # remove candidates containing stopwords\n",
    "    if set(extractor.stoplist) & set(words):\n",
    "        del extractor.candidates[candidate]\n",
    "\n",
    "# let's see how many candidates are identified\n",
    "print(\"{} keyphrase candidates were identified\".format(len(extractor.candidates)))\n",
    "\n",
    "# print out a sample\n",
    "candidates = [*extractor.candidates]\n",
    "print(\"- Subsample of candidates:\", ' ; '.join(candidates[:5]))\n",
    "\n",
    "# compute the maximum recall\n",
    "print(\"- Maximum recall: {:.3f}\".format(max_recall(candidates, references)))\n",
    "\n",
    "# identify missed reference keyphrases\n",
    "missed = set(references) - set(candidates)\n",
    "print(\"- Missed reference keyphrases: {}\".format(missed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa593ce7",
   "metadata": {},
   "source": [
    "### <span style=\"background:lightpink\">Exercice ✍️</span>\n",
    "\n",
    "try removing more spurious candidates to reduce the search space, for example by removing candidates containing punctuation marks as words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7b4fd",
   "metadata": {},
   "source": [
    "## Model parameterization - candidate weighting/ranking\n",
    "\n",
    "The keyphrase extraction model that we use in `pke` define how candidates are weighted. For example, in TopicRank, candidates are weighted using a graph-based ranking model whereas in YAKE, candidates are weighted using a combination of statistical features (e.g. position, frequency). Here, we will see how to use different models implemented in `pke`. For comparison purposes, we will use a unified candidate selection method based on the following PoS grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7881fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the unified grammar for candidate selection\n",
    "grammar=\"NP: {<ADJ>*<NOUN|PROPN>+}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5223a2ad",
   "metadata": {},
   "source": [
    "Models are evaluated against the gold-standard keyphrases by computing the precision, recall and f-measure at the top-N extracted keyphases as:\n",
    "\n",
    "$$ P@N = \\frac{| \\text{top-N keyphrases} \\cap \\text{references}|}{|\\text{top-N keyphrases}|} $$\n",
    "\n",
    "$$ R@N = \\frac{| \\text{top-N keyphrases} \\cap \\text{references}|}{|\\text{references}|} $$\n",
    "\n",
    "$$ F_1@N = \\frac{2 \\times P@N \\cdot R@N }{P@N + R@N} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b34e5286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(top_N_keyphrases, references):\n",
    "    P = len(set(top_N_keyphrases) & set(references)) / len(top_N_keyphrases)\n",
    "    R = len(set(top_N_keyphrases) & set(references)) / len(references)\n",
    "    F = (2*P*R)/(P+R) if (P+R) > 0 else 0 \n",
    "    return (P, R, F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe47114",
   "metadata": {},
   "source": [
    "### Baseline model: TopicRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65925f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-5 keyphrases: fundament paramet; nnfp; algorithm; classic theoret correct model; non-linear matrix effect\n",
      "P@5: 0.400 R@5: 0.118 F@5: 0.182\n"
     ]
    }
   ],
   "source": [
    "extractor = pke.unsupervised.TopicRank()\n",
    "extractor.load_document(input=sample, language='en')\n",
    "extractor.grammar_selection(grammar=grammar)\n",
    "extractor.candidate_weighting()\n",
    "keyphrases = extractor.get_n_best(n=5, stemming=True)\n",
    "\n",
    "top5 = [candidate for candidate, weight in keyphrases]\n",
    "print(\"top-5 keyphrases:\", '; '.join(top5))\n",
    "\n",
    "# evaluate the model\n",
    "P, R, F = evaluate(top5, references)\n",
    "print(\"P@5: {:.3f} R@5: {:.3f} F@5: {:.3f}\".format(P, R, F))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8428ef",
   "metadata": {},
   "source": [
    "### A strong baseline model: MultipartiteRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0cce756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-5 keyphrases: fundament paramet; algorithm; neural network; nnfp; classic theoret correct model\n",
      "P@5: 0.600 R@5: 0.176 F@5: 0.273\n"
     ]
    }
   ],
   "source": [
    "extractor = pke.unsupervised.MultipartiteRank()\n",
    "extractor.load_document(input=sample, language='en')\n",
    "extractor.grammar_selection(grammar=grammar)\n",
    "extractor.candidate_weighting()\n",
    "keyphrases = extractor.get_n_best(n=5, stemming=True)\n",
    "\n",
    "top5 = [candidate for candidate, weight in keyphrases]\n",
    "print(\"top-5 keyphrases:\", '; '.join(top5))\n",
    "\n",
    "# evaluate the model\n",
    "P, R, F = evaluate(top5, references)\n",
    "print(\"P@5: {:.3f} R@5: {:.3f} F@5: {:.3f}\".format(P, R, F))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25d04a",
   "metadata": {},
   "source": [
    "### <span style=\"background:lightpink\">Exercice ✍️</span>\n",
    "\n",
    "try using another model, for example among the other unsupervised models implemented in `pke`: `FirstPhrases`, `TextRank`, `TfIdf`, `YAKE` or a supervised model: `Kea`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
