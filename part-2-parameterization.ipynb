{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b764ffc9",
   "metadata": {},
   "source": [
    "# Hands-on session with pke\n",
    "\n",
    "## Part 2 : model parameterization\n",
    "\n",
    "As we have seen, keyphrase extraction is commonly treated as a three-stage process involving:\n",
    " 1. identifying keyphrase candidates\n",
    " 2. weighting these candidates\n",
    " 3. selecting the N-best candidates as keyphrases\n",
    "\n",
    "Each of the above stages can be parameterized/modified in `pke` and we will see how below.\n",
    "\n",
    "As a reminder, `pke` provides a standardized API for extracting keyphrases from a document by typing the following 5 lines:\n",
    "\n",
    "```python\n",
    "import pke\n",
    "\n",
    "extractor = pke.unsupervised.TopicRank()             # initialize a model, here TopicRank\n",
    "extractor.load_document(input='text', language='en') # load the document\n",
    "extractor.candidate_selection()                      # identify keyphrase candidates\n",
    "extractor.candidate_weighting()                      # weight candidates \n",
    "keyphrases = extractor.get_n_best(n=10)              # select the N-best candidates\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbc1f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke\n",
    "\n",
    "# sample document (C-41 from the SemEval-2010 dataset)\n",
    "sample = \"\"\"Evaluating Adaptive Resource Management for Distributed Real-Time Embedded Systems.\n",
    "A challenging problem faced by researchers and developers\n",
    "of distributed real-time and embedded (DRE) systems is \n",
    "devising and implementing effective adaptive resource \n",
    "management strategies that can meet end-to-end quality of service\n",
    "(QoS) requirements in varying operational conditions. This\n",
    "paper presents two contributions to research in adaptive \n",
    "resource management for DRE systems. First, we describe the\n",
    "structure and functionality of the Hybrid Adaptive \n",
    "Resourcemanagement Middleware (HyARM), which provides \n",
    "adaptive resource management using hybrid control techniques\n",
    "for adapting to workload fluctuations and resource \n",
    "availability. Second, we evaluate the adaptive behavior of HyARM\n",
    "via experiments on a DRE multimedia system that distributes\n",
    "video in real-time. Our results indicate that HyARM yields\n",
    "predictable, stable, and high system performance, even in the\n",
    "face of fluctuating workload and resource availability.\"\"\"\n",
    "\n",
    "# normalize spacing (replace newlines with whitespaces)\n",
    "sample = sample.replace(\"\\n\", \" \")\n",
    "\n",
    "# gold keyphrases for the sample document \n",
    "references = ['adapt resourc manag', 'distribut real-time embed system', \n",
    "              'end-to-end qualiti of servic', 'hybrid adapt resourcemanag middlewar',\n",
    "              'hybrid control techniqu', 'real-time video distribut system',\n",
    "              'real-time corba specif', 'video encod/decod', 'resourc reserv mechan',\n",
    "              'dynam environ', 'stream servic', 'distribut real-time emb system',\n",
    "              'hybrid system', 'qualiti of servic']\n",
    "\n",
    "# initialize a simple model that ranks candidates using their position\n",
    "extractor = pke.unsupervised.FirstPhrases()\n",
    "\n",
    "# load the document using the initialized model\n",
    "extractor.load_document(input=sample, language='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c9b2c",
   "metadata": {},
   "source": [
    "### Stage-1 : candidate selection\n",
    "\n",
    "Candidate selection is a crucial step in keyphrase extraction as it determines the size of the search space (i.e. number of candidates to rank) and the upper bound performance (i.e. maximum recall).\n",
    "Here, we will see how to configure the candidate selection method in `pke` to achieve the best compromise between search space and maximum performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b3525",
   "metadata": {},
   "source": [
    "#### Default's method (from FirstPhrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89738e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 keyphrase candidates were identified\n",
      "sample of candidates: adapt resourc manag ; real ; time embed system ; challeng problem ; research\n",
      "Maximum recall: 0.143\n"
     ]
    }
   ],
   "source": [
    "# First, we apply the default candidate selection method, which is model-dependent.\n",
    "# here for FirstPhrases, candidates are the longest sequences of nouns and adjectives\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# let's see how many candidates are identified\n",
    "print(\"{} keyphrase candidates were identified\".format(len(extractor.candidates)))\n",
    "\n",
    "# print out a sample\n",
    "candidates = [*extractor.candidates]\n",
    "print(\"sample of candidates:\", ' ; '.join(candidates[:5]))\n",
    "\n",
    "# compute the maximum recall\n",
    "max_recall = len(set(references) & set(candidates)) / len(set(references))\n",
    "print(\"Maximum recall: {:.3f}\".format(max_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9368d",
   "metadata": {},
   "source": [
    "#### Identify keyphrase candidates using a PoS patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bbdf69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 keyphrase candidates were identified\n",
      "sample of candidates: adapt resourc manag ; real ; time embed system ; challeng problem ; research\n",
      "Maximum recall: 0.143\n"
     ]
    }
   ],
   "source": [
    "# first we need to empty the candidates\n",
    "extractor.candidates.clear()\n",
    "\n",
    "# here we use a simple grammar for candidates (NP) as\n",
    "grammar = r\"\"\"\n",
    "                NBAR:\n",
    "                    {<NOUN|PROPN|ADJ>{,2}<NOUN|PROPN>} \n",
    "                    \n",
    "                NP:\n",
    "                    {<NBAR>}\n",
    "                    {<NBAR><ADP><NBAR>}\n",
    "            \"\"\"\n",
    "\n",
    "extractor.grammar_selection(grammar=grammar)\n",
    "\n",
    "# let's see how many candidates are identified\n",
    "print(\"{} keyphrase candidates were identified\".format(len(extractor.candidates)))\n",
    "\n",
    "# print out a sample\n",
    "candidates = [*extractor.candidates]\n",
    "print(\"sample of candidates:\", ' ; '.join(candidates[:5]))\n",
    "\n",
    "# compute the maximum recall\n",
    "max_recall = len(set(references) & set(candidates)) / len(set(references))\n",
    "print(\"Maximum recall: {:.3f}\".format(max_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c502c893",
   "metadata": {},
   "source": [
    "#### Identify n-grams as keyphrase candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74271ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378 keyphrase candidates were identified\n",
      "sample of candidates: evalu ; evalu adapt ; evalu adapt resourc ; adapt ; adapt resourc\n",
      "Maximum recall: 0.214\n"
     ]
    }
   ],
   "source": [
    "# first we need to empty the candidates\n",
    "extractor.candidates.clear()\n",
    "\n",
    "# here we use a simple n-gram selection for candidates\n",
    "extractor.ngram_selection(n=3)\n",
    "\n",
    "# let's see how many candidates are identified\n",
    "print(\"{} keyphrase candidates were identified\".format(len(extractor.candidates)))\n",
    "\n",
    "# print out a sample\n",
    "candidates = [*extractor.candidates]\n",
    "print(\"sample of candidates:\", ' ; '.join(candidates[:5]))\n",
    "\n",
    "# compute the maximum recall\n",
    "max_recall = len(set(references) & set(candidates)) / len(set(references))\n",
    "print(\"Maximum recall: {:.3f}\".format(max_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7b4fd",
   "metadata": {},
   "source": [
    "### Stage-2 candidate weighting/ranking\n",
    "\n",
    "The keyphrase extraction model that we use in `pke` define how keyphrase candidates are weighted. For example, in TopicRank, candidates are weighted using a graph-based ranking model whereas in Yake, candidates are weighted using a combination of statistical features (e.g. position, frequency). Here, we will see how to use different models implemented in `pke`. For comparison purposes, we will use a unified candidate selection method (as presented above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe47114",
   "metadata": {},
   "source": [
    "#### Baseline model: TopicRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65925f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyphrases: adapt resourc manag ; dre ; hyarm ; time embed system ; hybrid adapt\n",
      "P@5: 0.200 R@5: 0.071 F@5: 0.105\n"
     ]
    }
   ],
   "source": [
    "# initialize a TopicRank model\n",
    "extractor = pke.unsupervised.TopicRank()\n",
    "\n",
    "# load the document\n",
    "extractor.load_document(input=sample, language='en')\n",
    "\n",
    "# identify Noun Phrases as keyphrase candidates\n",
    "extractor.grammar_selection(grammar=\"NP: {<ADJ>*<NOUN|PROPN>+}\")\n",
    "\n",
    "# weight candidates\n",
    "extractor.candidate_weighting(threshold=0.74) # the threshold parameter is used to compute topics\n",
    "\n",
    "# get the 5 best keyphrases\n",
    "keyphrases = [candidate for candidate, weight in extractor.get_n_best(n=5, stemming=True)]\n",
    "print(\"keyphrases:\", ' ; '.join(keyphrases))\n",
    "\n",
    "# evaluate the Precision / Recall / F-measure of the model\n",
    "P = len(set(keyphrases) & set(references)) / len(keyphrases)\n",
    "R = len(set(keyphrases) & set(references)) / len(references)\n",
    "F = 2 * (P*R) / (P+R)\n",
    "print(\"P@5: {:.3f} R@5: {:.3f} F@5: {:.3f}\".format(P, R, F))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8428ef",
   "metadata": {},
   "source": [
    "#### Good performance graph-based model: MultipartiteRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0cce756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyphrases: adapt resourc manag ; time embed system ; dre ; hyarm ; hybrid adapt\n",
      "P@5: 0.200 R@5: 0.071 F@5: 0.105\n"
     ]
    }
   ],
   "source": [
    "# initialize a MultipartiteRank model\n",
    "extractor = pke.unsupervised.MultipartiteRank()\n",
    "\n",
    "# load the document\n",
    "extractor.load_document(input=sample, language='en')\n",
    "\n",
    "# identify Noun Phrases as keyphrase candidates\n",
    "extractor.grammar_selection(grammar=\"NP: {<ADJ>*<NOUN|PROPN>+}\")\n",
    "\n",
    "# weight candidates\n",
    "extractor.candidate_weighting(threshold=0.74,  # parameter used to compute topics\n",
    "                              alpha=1.1)       # parameter that controls the strength of the weight adjustment\n",
    "\n",
    "# get the 5 best keyphrases\n",
    "keyphrases = [candidate for candidate, weight in extractor.get_n_best(n=5, stemming=True)]\n",
    "print(\"keyphrases:\", ' ; '.join(keyphrases))\n",
    "\n",
    "\n",
    "# evaluate the Precision / Recall / F-measure of the model\n",
    "P = len(set(keyphrases) & set(references)) / len(keyphrases)\n",
    "R = len(set(keyphrases) & set(references)) / len(references)\n",
    "F = 2 * (P*R) / (P+R)\n",
    "print(\"P@5: {:.3f} R@5: {:.3f} F@5: {:.3f}\".format(P, R, F))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c4f711",
   "metadata": {},
   "source": [
    "#### Supervised model: Kea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68245b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:LoadFile._df_counts is hard coded to /usr/local/lib/python3.9/site-packages/pke/models/df-semeval2010.tsv.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyphrases: hyarm ; adapt resourc manag ; time embed system ; dre ; effect adapt resourc\n",
      "P@5: 0.200 R@5: 0.071 F@5: 0.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/base.py:324: UserWarning: Trying to unpickle estimator MultinomialNB from version 0.20.0 when using version 1.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# initialize a TopicRank model\n",
    "extractor = pke.supervised.Kea()\n",
    "\n",
    "# load the document\n",
    "extractor.load_document(input=sample, language='en')\n",
    "\n",
    "# identify Noun Phrases as keyphrase candidates\n",
    "extractor.grammar_selection(grammar=\"NP: {<ADJ>*<NOUN|PROPN>+}\")\n",
    "\n",
    "# weight candidates\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# get the 5 best keyphrases\n",
    "keyphrases = [candidate for candidate, weight in extractor.get_n_best(n=5, stemming=True)]\n",
    "print(\"keyphrases:\", ' ; '.join(keyphrases))\n",
    "\n",
    "\n",
    "# evaluate the Precision / Recall / F-measure of the model\n",
    "P = len(set(keyphrases) & set(references)) / len(keyphrases)\n",
    "R = len(set(keyphrases) & set(references)) / len(references)\n",
    "F = 2 * (P*R) / (P+R)\n",
    "print(\"P@5: {:.3f} R@5: {:.3f} F@5: {:.3f}\".format(P, R, F))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da805a7",
   "metadata": {},
   "source": [
    "### Stage-3 selecting the N-best candidates\n",
    "\n",
    "In this last stage, the highest weighted keyphrase candidates are selected as output keyphrases. Here, we will see how to configure this selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31315c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-10 keyphrases\n",
      "adaptive resource management:0.08897405365022867\n",
      "time embedded systems:0.05966042113023676\n",
      "dre:0.05322017865856586\n",
      "hyarm:0.046889644710346554\n",
      "hybrid adaptive:0.036936501953574175\n",
      "end:0.034285447267345746\n",
      "systems:0.03426425286057176\n",
      "time:0.0317259787561323\n",
      "real:0.031146977659670304\n",
      "workload fluctuations:0.02882190551192987\n",
      "**********\n",
      "5-best in stemmed forms [('adapt resourc manag', 0.08897405365022867), ('time embed system', 0.05966042113023676), ('dre', 0.05322017865856586), ('hyarm', 0.046889644710346554), ('hybrid adapt', 0.036936501953574175), ('end', 0.034285447267345746), ('system', 0.03426425286057176), ('time', 0.0317259787561323), ('real', 0.031146977659670304), ('workload fluctuat', 0.02882190551192987)]\n",
      "5-best in stemmed forms w/o redundancy [('adapt resourc manag', 0.08897405365022867), ('time embed system', 0.05966042113023676), ('dre', 0.05322017865856586), ('hyarm', 0.046889644710346554), ('hybrid adapt', 0.036936501953574175), ('end', 0.034285447267345746), ('real', 0.031146977659670304), ('workload fluctuat', 0.02882190551192987), ('challeng problem', 0.027218143863136902), ('hybrid control techniqu', 0.026488359697676392)]\n"
     ]
    }
   ],
   "source": [
    "# initialize a MultipartiteRank model\n",
    "extractor = pke.unsupervised.MultipartiteRank()\n",
    "\n",
    "# load the document\n",
    "extractor.load_document(input=sample, language='en')\n",
    "\n",
    "# identify Noun Phrases as keyphrase candidates\n",
    "extractor.grammar_selection(grammar=\"NP: {<ADJ>*<NOUN|PROPN>+}\")\n",
    "\n",
    "# weight candidates\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# print out the 10 best candidates\n",
    "print(\"top-10 keyphrases\")\n",
    "print(\"\\n\".join([ \"{}:{}\".format(u, v) for u, v in extractor.get_n_best(n=10)]))\n",
    "print(\"*\"*10)\n",
    "\n",
    "# same in stemmed forms\n",
    "print('5-best in stemmed forms', extractor.get_n_best(n=10, stemming=True))\n",
    "\n",
    "# same but with redundancy removal (removing candidates contained in higher-ranked ones)\n",
    "print('5-best in stemmed forms w/o redundancy', extractor.get_n_best(n=10, redundancy_removal=True, stemming=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5924b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
